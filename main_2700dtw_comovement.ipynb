{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c61c43d-142c-4a56-b7a5-1fcc06372ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "데이터 로드 및 월별 집계\n",
      "================================================================================\n",
      "데이터 로드 완료: 100개 품목, 43개월\n",
      "\n",
      "================================================================================\n",
      " DTW 기반 pairs → 2,700개 품질 필터 → 32피처 LGBM+XGB 앙상블\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding pairs (DTW): 100%|███████████████████████████████████████████████████████████| 100/100 [00:46<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pairs with DTW: 3368개\n",
      "  leading_item_id following_item_id  best_lag  max_corr   p_value  \\\n",
      "0        AANGBULD          ZCELVYQU         7  0.677701  0.000006   \n",
      "1        AANGBULD          ZKENOUDA         1  0.599969  0.000027   \n",
      "2        AANGBULD          DEWLVASR         6  0.640221  0.000020   \n",
      "3        AANGBULD          NAQIHUKZ         2  0.525490  0.000419   \n",
      "4        AANGBULD          GKQIJYDH         6  0.582501  0.000155   \n",
      "\n",
      "   dtw_distance  comovement_score  \n",
      "0      0.304327        120.909908  \n",
      "1      0.513452        119.569775  \n",
      "2      0.490232        114.993153  \n",
      "3      0.471456        112.170840  \n",
      "4      0.464203        109.014231  \n",
      "\n",
      "================================================================================\n",
      "2,700개 목표 노이즈 제거 (중간 모드)\n",
      "================================================================================\n",
      "원본: 3,368개\n",
      "목표: 2,700개 (약 80% 유지)\n",
      "\n",
      "품질 평가 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가: 100%|███████████████████████████████████████████████████████████████████████| 3368/3368 [00:04<00:00, 721.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "노이즈 제거 완료!\n",
      "================================================================================\n",
      "원본: 3,368개\n",
      "제거: 611개 (18.1%)\n",
      "유지: 2,757개 (81.9%)\n",
      "임계값: 55점\n",
      "\n",
      "품질 점수 분포:\n",
      "  - 평균: 65.5점\n",
      "  - 중앙값: 67.0점\n",
      "  - 최소: 36.0점\n",
      "  - 최대: 90.0점\n",
      "\n",
      "선택된 쌍의 점수:\n",
      "  - 평균: 69.2점\n",
      "  - 최소: 55.0점\n",
      "\n",
      "점수 구간별 분포:\n",
      "    0- 30점: 전체    0개, 선택    0개\n",
      "   30- 40점: 전체   29개, 선택    0개\n",
      "   40- 50점: 전체  205개, 선택    0개\n",
      "   50- 60점: 전체  708개, 선택  331개\n",
      "   60- 70점: 전체 1,002개, 선택 1,002개\n",
      "   70- 80점: 전체 1,212개, 선택 1,212개\n",
      "   80- 90점: 전체  208개, 선택  208개\n",
      "   90-100점: 전체    4개, 선택    4개\n",
      "\n",
      "기준별 평균 점수 (선택된 쌍):\n",
      "  0 비율           : 평균   9.8점 (최대 10점)\n",
      "  변동계수           : 평균   7.8점 (최대 10점)\n",
      "  스파이크           : 평균   6.2점 (최대 10점)\n",
      "  안정성            : 평균   5.0점 (최대 15점)\n",
      "  트렌드 일치         : 평균   3.9점 (최대 10점)\n",
      "  방향 일치도         : 평균  10.0점 (최대 10점)\n",
      "  통계적 유의성        : 평균   7.7점 (최대 10점)\n",
      "  자기상관           : 평균   7.8점 (최대 10점)\n",
      "  이상치 비율         : 평균   8.7점 (최대 10점)\n",
      "  정규성            : 평균   2.2점 (최대 5점)\n",
      "\n",
      "최종 사용 쌍: 2757개\n",
      "\n",
      "================================================================================\n",
      "학습 데이터 생성\n",
      "================================================================================\n",
      "✓ 학습 데이터: (82710, 32)\n",
      "\n",
      "================================================================================\n",
      "모델 학습\n",
      "================================================================================\n",
      "학습 샘플: 82,710, 피처: 31\n",
      "\n",
      "LGBM 학습 중...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6428\n",
      "[LightGBM] [Info] Number of data points in the train set: 82710, number of used features: 31\n",
      "[LightGBM] [Info] Start training from score 13.019085\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "✓ LGBM 완료\n",
      "XGBoost 학습 중...\n",
      "✓ XGBoost 완료\n",
      "\n",
      "================================================================================\n",
      "예측\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "예측: 2757it [00:14, 191.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ 예측 완료: 2757개\n",
      "\n",
      " 최종 완료!\n",
      "  - 저장 위치: _result/jh_2700dtw_20251128_163251.csv\n",
      "  - Stage1: Corr+DTW pairs → 2,700개 품질 필터\n",
      "  - Stage2: 32개 피처 + log1p target\n",
      "  - Stage3: LGBM + XGB 앙상블 (0.7 / 0.3)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# =============================================================================\n",
    "# 0. 데이터 로드 & 월별 집계 (value, weight)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"데이터 로드 및 월별 집계\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "train = pd.read_csv(\"./data/train.csv\")\n",
    "\n",
    "monthly = (\n",
    "    train\n",
    "    .groupby([\"item_id\", \"year\", \"month\"], as_index=False)\n",
    "    .agg({'value': 'sum', 'weight': 'sum'})\n",
    ")\n",
    "\n",
    "monthly[\"ym\"] = pd.to_datetime(\n",
    "    monthly[\"year\"].astype(str) + \"-\" + monthly[\"month\"].astype(str).str.zfill(2)\n",
    ")\n",
    "\n",
    "pivot = monthly.pivot(index=\"item_id\", columns=\"ym\", values=\"value\").fillna(0.0)\n",
    "pivot_weight = monthly.pivot(index=\"item_id\", columns=\"ym\", values=\"weight\").fillna(0.0)\n",
    "months_dt = pivot.columns.to_list()\n",
    "\n",
    "print(f\"데이터 로드 완료: {len(pivot)}개 품목, {len(months_dt)}개월\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Corr + DTW 기반 comovement pair 탐색 (pairs_with_dtw)\n",
    "# =============================================================================\n",
    "\n",
    "def safe_corr_with_pvalue(x, y):\n",
    "    if np.std(x) == 0 or np.std(y) == 0:\n",
    "        return 0.0, 1.0\n",
    "    corr, p_value = pearsonr(x, y)\n",
    "    return float(corr), float(p_value)\n",
    "\n",
    "\n",
    "def calculate_score_advanced(corr, p_value, lag):\n",
    "    \"\"\"\n",
    "    상관계수 + p-value + lag 기반 base score (0~100 정도 스케일)\n",
    "    \"\"\"\n",
    "    # 상관계수 (0~70점)\n",
    "    corr_score = abs(corr) * 70\n",
    "\n",
    "    # 유의성 점수\n",
    "    if p_value < 0.001:\n",
    "        sig_score = 20\n",
    "    elif p_value < 0.01:\n",
    "        sig_score = 15\n",
    "    elif p_value < 0.05:\n",
    "        sig_score = 10\n",
    "    else:\n",
    "        sig_score = 5\n",
    "\n",
    "    # Lag 점수\n",
    "    if lag <= 2:\n",
    "        lag_score = 10\n",
    "    elif lag <= 4:\n",
    "        lag_score = 7\n",
    "    elif lag <= 6:\n",
    "        lag_score = 4\n",
    "    else:\n",
    "        lag_score = 1\n",
    "\n",
    "    return corr_score + sig_score + lag_score\n",
    "\n",
    "\n",
    "def dtw_distance(series_A, series_B):\n",
    "    \"\"\"\n",
    "    Dynamic Time Warping 거리 계산\n",
    "    \"\"\"\n",
    "    n, m = len(series_A), len(series_B)\n",
    "    dtw_matrix = np.full((n + 1, m + 1), np.inf)\n",
    "    dtw_matrix[0, 0] = 0\n",
    "\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(1, m + 1):\n",
    "            cost = abs(series_A[i-1] - series_B[j-1])\n",
    "            dtw_matrix[i, j] = cost + min(\n",
    "                dtw_matrix[i-1, j],\n",
    "                dtw_matrix[i, j-1],\n",
    "                dtw_matrix[i-1, j-1]\n",
    "            )\n",
    "\n",
    "    return dtw_matrix[n, m]\n",
    "\n",
    "\n",
    "def calculate_normalized_dtw(x, y, lag):\n",
    "    \"\"\"\n",
    "    정규화 + DTW 거리 계산\n",
    "    x: 선행 품목\n",
    "    y: 후행 품목\n",
    "    lag: 시차\n",
    "    \"\"\"\n",
    "    if len(x) <= lag or len(y) <= lag:\n",
    "        return float('inf')\n",
    "\n",
    "    x_aligned = x[:-lag]\n",
    "    y_aligned = y[lag:]\n",
    "\n",
    "    if len(x_aligned) < 5:\n",
    "        return float('inf')\n",
    "\n",
    "    # 정규화\n",
    "    scaler = StandardScaler()\n",
    "    normalized_x = scaler.fit_transform(x_aligned.reshape(-1, 1)).flatten()\n",
    "    normalized_y = scaler.fit_transform(y_aligned.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # DTW 거리\n",
    "    distance = dtw_distance(normalized_x, normalized_y)\n",
    "\n",
    "    # 길이로 정규화\n",
    "    normalized_distance = distance / len(normalized_x)\n",
    "    return normalized_distance\n",
    "\n",
    "\n",
    "def find_comovement_pairs_with_dtw(\n",
    "    pivot,\n",
    "    max_lag=9,\n",
    "    min_nonzero=12,\n",
    "    corr_threshold=0.33,\n",
    "    score_threshold=40,\n",
    "    use_dtw=True,\n",
    "    dtw_threshold=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    corr + p-value + lag + (선택적 DTW) 기반 comovement pair 탐색\n",
    "    - 출력: pairs_with_dtw\n",
    "      [leading_item_id, following_item_id, best_lag, max_corr, p_value, dtw_distance, comovement_score]\n",
    "    \"\"\"\n",
    "    items = pivot.index.to_list()\n",
    "    months = pivot.columns.to_list()\n",
    "    n_months = len(months)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i, leader in tqdm(enumerate(items), total=len(items), desc=\"Finding pairs (DTW)\"):\n",
    "        x = pivot.loc[leader].values.astype(float)\n",
    "        if np.count_nonzero(x) < min_nonzero:\n",
    "            continue\n",
    "\n",
    "        candidates = []\n",
    "\n",
    "        for follower in items:\n",
    "            if follower == leader:\n",
    "                continue\n",
    "\n",
    "            y = pivot.loc[follower].values.astype(float)\n",
    "            if np.count_nonzero(y) < min_nonzero:\n",
    "                continue\n",
    "\n",
    "            best_lag = None\n",
    "            best_corr = 0.0\n",
    "            best_p_value = 1.0\n",
    "            best_dtw = float('inf')\n",
    "\n",
    "            # lag 탐색\n",
    "            for lag in range(1, max_lag + 1):\n",
    "                if n_months <= lag:\n",
    "                    continue\n",
    "\n",
    "                corr, p_value = safe_corr_with_pvalue(x[:-lag], y[lag:])\n",
    "\n",
    "                if abs(corr) > abs(best_corr):\n",
    "                    best_corr = corr\n",
    "                    best_lag = lag\n",
    "                    best_p_value = p_value\n",
    "\n",
    "            # 1단계: 상관계수 임계값 통과 시\n",
    "            if best_lag is not None and abs(best_corr) >= corr_threshold:\n",
    "\n",
    "                # 2단계: DTW 검증 (옵션)\n",
    "                if use_dtw:\n",
    "                    best_dtw = calculate_normalized_dtw(x, y, best_lag)\n",
    "\n",
    "                    # DTW 임계값 통과 못하면 스킵\n",
    "                    if best_dtw > dtw_threshold:\n",
    "                        continue\n",
    "\n",
    "                # 종합 점수 계산\n",
    "                score = calculate_score_advanced(best_corr, best_p_value, best_lag)\n",
    "\n",
    "                # DTW 점수 반영 (옵션)\n",
    "                if use_dtw and best_dtw != float('inf'):\n",
    "                    # DTW가 낮을수록 좋으므로 역수 사용\n",
    "                    dtw_bonus = 1 / (1 + best_dtw)\n",
    "                    score = score * (1 + dtw_bonus)  # DTW 보너스 추가\n",
    "\n",
    "                # 점수 임계값 통과 시 후보에 추가\n",
    "                if score >= score_threshold:\n",
    "                    candidates.append({\n",
    "                        \"following_item_id\": follower,\n",
    "                        \"best_lag\": best_lag,\n",
    "                        \"max_corr\": best_corr,\n",
    "                        \"p_value\": best_p_value,\n",
    "                        \"dtw_distance\": best_dtw if use_dtw else None,\n",
    "                        \"comovement_score\": score\n",
    "                    })\n",
    "\n",
    "        # 점수 순으로 정렬 (높은 순)\n",
    "        candidates.sort(key=lambda x: -x['comovement_score'])\n",
    "\n",
    "        for candidate in candidates:\n",
    "            result_dict = {\n",
    "                \"leading_item_id\": leader,\n",
    "                \"following_item_id\": candidate[\"following_item_id\"],\n",
    "                \"best_lag\": candidate[\"best_lag\"],\n",
    "                \"max_corr\": candidate[\"max_corr\"],\n",
    "                \"p_value\": candidate[\"p_value\"],\n",
    "                \"dtw_distance\": candidate[\"dtw_distance\"],\n",
    "                \"comovement_score\": candidate[\"comovement_score\"],\n",
    "            }\n",
    "\n",
    "            results.append(result_dict)\n",
    "\n",
    "    pairs = pd.DataFrame(results)\n",
    "    print(f\"\\nPairs with DTW: {len(pairs)}개\")\n",
    "    print(pairs.head())\n",
    "    return pairs\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. 2,700개 목표 노이즈 제거 \n",
    "# =============================================================================\n",
    "\n",
    "def calculate_quality_score_moderate(a_series, b_series, lag):\n",
    "    \"\"\"\n",
    "    중간 품질 점수 계산 (0-100점)\n",
    "    - 원본 3,000대 → 목표 2,700개\n",
    "    - 적절한 노이즈 제거 (80% 유지)\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "\n",
    "    # 1. 0 비율 (10점) - 중간\n",
    "    zero_ratio_a = (a_series == 0).sum() / len(a_series)\n",
    "    zero_ratio_b = (b_series == 0).sum() / len(b_series)\n",
    "    max_zero = max(zero_ratio_a, zero_ratio_b)\n",
    "\n",
    "    if max_zero < 0.17:\n",
    "        scores['zero'] = 10\n",
    "    elif max_zero < 0.37:\n",
    "        scores['zero'] = 7\n",
    "    elif max_zero < 0.57:  # 57%까지 허용\n",
    "        scores['zero'] = 4\n",
    "    else:\n",
    "        scores['zero'] = 0\n",
    "\n",
    "    # 2. 변동계수 (10점) - 중간\n",
    "    cv_a = np.std(a_series) / (np.mean(a_series) + 1)\n",
    "    cv_b = np.std(b_series) / (np.mean(b_series) + 1)\n",
    "    max_cv = max(cv_a, cv_b)\n",
    "\n",
    "    if max_cv < 0.75:\n",
    "        scores['cv'] = 10\n",
    "    elif max_cv < 1.4:\n",
    "        scores['cv'] = 7\n",
    "    elif max_cv < 2.7:  # 2.7까지 허용\n",
    "        scores['cv'] = 4\n",
    "    else:\n",
    "        scores['cv'] = 0\n",
    "\n",
    "    # 3. 스파이크 (10점) - 중간\n",
    "    a_changes = np.abs(np.diff(a_series))\n",
    "    b_changes = np.abs(np.diff(b_series))\n",
    "\n",
    "    if len(a_changes) > 0 and len(b_changes) > 0:\n",
    "        a_spike = a_changes.max() / (np.median(a_changes) + 1)\n",
    "        b_spike = b_changes.max() / (np.median(b_changes) + 1)\n",
    "        max_spike = max(a_spike, b_spike)\n",
    "\n",
    "        if max_spike < 4.5:\n",
    "            scores['spike'] = 10\n",
    "        elif max_spike < 9.5:\n",
    "            scores['spike'] = 7\n",
    "        elif max_spike < 17:  # 17까지 허용\n",
    "            scores['spike'] = 4\n",
    "        else:\n",
    "            scores['spike'] = 0\n",
    "    else:\n",
    "        scores['spike'] = 5\n",
    "\n",
    "    # 4. 안정성 (15점) - 중간\n",
    "    n = len(a_series)\n",
    "\n",
    "    try:\n",
    "        mid = n // 2\n",
    "\n",
    "        if mid >= 12 and mid + lag < len(b_series):\n",
    "            corr_first = np.corrcoef(\n",
    "                a_series[:mid],\n",
    "                b_series[lag:mid+lag]\n",
    "            )[0, 1]\n",
    "\n",
    "            corr_second = np.corrcoef(\n",
    "                a_series[mid:],\n",
    "                b_series[mid+lag:]\n",
    "            )[0, 1]\n",
    "\n",
    "            if not np.isnan(corr_first) and not np.isnan(corr_second):\n",
    "                consistency = 1 - abs(corr_first - corr_second)\n",
    "\n",
    "                if consistency > 0.52:  # 중간\n",
    "                    scores['stability'] = 15\n",
    "                elif consistency > 0.35:\n",
    "                    scores['stability'] = 10\n",
    "                elif consistency > 0.22:\n",
    "                    scores['stability'] = 6\n",
    "                else:\n",
    "                    scores['stability'] = 3\n",
    "            else:\n",
    "                scores['stability'] = 5\n",
    "        else:\n",
    "            scores['stability'] = 5\n",
    "    except:\n",
    "        scores['stability'] = 5\n",
    "\n",
    "    # 5. 트렌드 일치 (10점) - 중간\n",
    "    if len(a_series) > 0 and len(b_series) > 0:\n",
    "        x = np.arange(len(a_series))\n",
    "\n",
    "        if np.std(a_series) > 0:\n",
    "            slope_a, _ = np.polyfit(x, a_series, 1)\n",
    "        else:\n",
    "            slope_a = 0\n",
    "\n",
    "        if np.std(b_series) > 0:\n",
    "            slope_b, _ = np.polyfit(x, b_series, 1)\n",
    "        else:\n",
    "            slope_b = 0\n",
    "\n",
    "        if (slope_a > 0 and slope_b > 0) or (slope_a < 0 and slope_b < 0):\n",
    "            ratio = abs(slope_a) / (abs(slope_b) + 1e-10)\n",
    "            if 0.38 < ratio < 2.7:  # 중간\n",
    "                scores['trend'] = 10\n",
    "            elif 0.25 < ratio < 4.2:\n",
    "                scores['trend'] = 6\n",
    "            else:\n",
    "                scores['trend'] = 3\n",
    "        elif slope_a == 0 or slope_b == 0:\n",
    "            scores['trend'] = 5\n",
    "        else:\n",
    "            scores['trend'] = 2\n",
    "    else:\n",
    "        scores['trend'] = 4\n",
    "\n",
    "    # 6. 방향 일치도 (10점) - 중간\n",
    "    if len(a_changes) > lag and len(b_changes) > lag:\n",
    "        direction_match = (np.sign(a_changes[:-lag]) == np.sign(b_changes[lag:])).mean()\n",
    "\n",
    "        if direction_match > 0.64:  # 중간\n",
    "            scores['direction'] = 10\n",
    "        elif direction_match > 0.54:\n",
    "            scores['direction'] = 7\n",
    "        elif direction_match > 0.44:\n",
    "            scores['direction'] = 4\n",
    "        else:\n",
    "            scores['direction'] = 1\n",
    "    else:\n",
    "        scores['direction'] = 4\n",
    "\n",
    "    # 7. 통계적 유의성 (10점) - 중간\n",
    "    try:\n",
    "        if len(a_series) > lag:\n",
    "            _, p_value = stats.pearsonr(\n",
    "                a_series[:-lag] if lag > 0 else a_series,\n",
    "                b_series[lag:] if lag > 0 else b_series\n",
    "            )\n",
    "\n",
    "            if p_value < 0.007:  # 중간\n",
    "                scores['significance'] = 10\n",
    "            elif p_value < 0.025:\n",
    "                scores['significance'] = 7\n",
    "            elif p_value < 0.07:\n",
    "                scores['significance'] = 4\n",
    "            else:\n",
    "                scores['significance'] = 1\n",
    "        else:\n",
    "            scores['significance'] = 4\n",
    "    except:\n",
    "        scores['significance'] = 4\n",
    "\n",
    "    # 8. 자기상관 (10점) - 중간\n",
    "    try:\n",
    "        if len(b_series) > 1:\n",
    "            autocorr = np.corrcoef(b_series[:-1], b_series[1:])[0, 1]\n",
    "\n",
    "            if 0.24 < autocorr < 0.86:  # 중간\n",
    "                scores['autocorr'] = 10\n",
    "            elif 0.09 < autocorr < 0.91:\n",
    "                scores['autocorr'] = 7\n",
    "            else:\n",
    "                scores['autocorr'] = 3\n",
    "        else:\n",
    "            scores['autocorr'] = 4\n",
    "    except:\n",
    "        scores['autocorr'] = 4\n",
    "\n",
    "    # 9. 이상치 비율 (10점) - 중간\n",
    "    def count_outliers(series):\n",
    "        q1 = np.percentile(series, 25)\n",
    "        q3 = np.percentile(series, 75)\n",
    "        iqr = q3 - q1\n",
    "        lower = q1 - 1.5 * iqr\n",
    "        upper = q3 + 1.5 * iqr\n",
    "        outliers = ((series < lower) | (series > upper)).sum()\n",
    "        return outliers / len(series)\n",
    "\n",
    "    outlier_ratio_a = count_outliers(a_series)\n",
    "    outlier_ratio_b = count_outliers(b_series)\n",
    "    max_outlier = max(outlier_ratio_a, outlier_ratio_b)\n",
    "\n",
    "    if max_outlier < 0.085:\n",
    "        scores['outlier'] = 10\n",
    "    elif max_outlier < 0.16:\n",
    "        scores['outlier'] = 7\n",
    "    elif max_outlier < 0.26:  # 26%까지 허용\n",
    "        scores['outlier'] = 4\n",
    "    else:\n",
    "        scores['outlier'] = 1\n",
    "\n",
    "    # 10. 분포 정규성 (5점) - 중간\n",
    "    try:\n",
    "        if len(b_series) >= 8:\n",
    "            _, p_shapiro = stats.shapiro(b_series)\n",
    "\n",
    "            if p_shapiro > 0.02:  # 중간\n",
    "                scores['normality'] = 5\n",
    "            elif p_shapiro > 0.004:\n",
    "                scores['normality'] = 3\n",
    "            else:\n",
    "                scores['normality'] = 1\n",
    "        else:\n",
    "            scores['normality'] = 2\n",
    "    except:\n",
    "        scores['normality'] = 2\n",
    "\n",
    "    total_score = sum(scores.values())\n",
    "    return total_score, scores\n",
    "\n",
    "\n",
    "def remove_noise_pairs_for_2700(pairs, pivot, target_count=2700):\n",
    "    \"\"\"\n",
    "    2,700개 목표 노이즈 제거 (중간 모드)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"2,700개 목표 노이즈 제거 (중간 모드)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"원본: {len(pairs):,}개\")\n",
    "    print(f\"목표: {target_count:,}개 (약 {target_count/len(pairs)*100:.0f}% 유지)\")\n",
    "\n",
    "    quality_scores = []\n",
    "    detailed_scores = []\n",
    "\n",
    "    # 품질 점수 계산 (중간 기준)\n",
    "    print(f\"\\n품질 평가 중...\")\n",
    "    for row in tqdm(pairs.itertuples(), total=len(pairs), desc=\"평가\"):\n",
    "        leader = row.leading_item_id\n",
    "        follower = row.following_item_id\n",
    "        lag = int(row.best_lag)\n",
    "\n",
    "        if leader not in pivot.index or follower not in pivot.index:\n",
    "            quality_scores.append(0)\n",
    "            detailed_scores.append({})\n",
    "            continue\n",
    "\n",
    "        a_series = pivot.loc[leader].values\n",
    "        b_series = pivot.loc[follower].values\n",
    "\n",
    "        total_score, scores = calculate_quality_score_moderate(a_series, b_series, lag)\n",
    "\n",
    "        quality_scores.append(total_score)\n",
    "        detailed_scores.append(scores)\n",
    "\n",
    "    # 품질 점수 추가\n",
    "    pairs_with_score = pairs.copy()\n",
    "    pairs_with_score['quality_score'] = quality_scores\n",
    "\n",
    "    # 목표 개수 기반 임계값 계산\n",
    "    sorted_scores = sorted(quality_scores, reverse=True)\n",
    "\n",
    "    if len(sorted_scores) >= target_count:\n",
    "        threshold = sorted_scores[target_count - 1]\n",
    "        threshold = max(33, (threshold // 5) * 5)  # 최소 33점, 5점 단위 내림\n",
    "    else:\n",
    "        threshold = 33\n",
    "\n",
    "    # 필터링\n",
    "    pairs_clean = pairs_with_score[\n",
    "        pairs_with_score['quality_score'] >= threshold\n",
    "    ].copy()\n",
    "\n",
    "    # 통계\n",
    "    removed = len(pairs) - len(pairs_clean)\n",
    "    removed_ratio = removed / len(pairs) * 100\n",
    "    kept_ratio = len(pairs_clean) / len(pairs) * 100\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"노이즈 제거 완료!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"원본: {len(pairs):,}개\")\n",
    "    print(f\"제거: {removed:,}개 ({removed_ratio:.1f}%)\")\n",
    "    print(f\"유지: {len(pairs_clean):,}개 ({kept_ratio:.1f}%)\")\n",
    "    print(f\"임계값: {threshold}점\")\n",
    "\n",
    "    # 점수 분포\n",
    "    print(f\"\\n품질 점수 분포:\")\n",
    "    print(f\"  - 평균: {np.mean(quality_scores):.1f}점\")\n",
    "    print(f\"  - 중앙값: {np.median(quality_scores):.1f}점\")\n",
    "    print(f\"  - 최소: {np.min(quality_scores):.1f}점\")\n",
    "    print(f\"  - 최대: {np.max(quality_scores):.1f}점\")\n",
    "\n",
    "    selected_scores = pairs_clean['quality_score'].values\n",
    "    print(f\"\\n선택된 쌍의 점수:\")\n",
    "    print(f\"  - 평균: {np.mean(selected_scores):.1f}점\")\n",
    "    print(f\"  - 최소: {np.min(selected_scores):.1f}점\")\n",
    "\n",
    "    print(f\"\\n점수 구간별 분포:\")\n",
    "    bins = [0, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "    for i in range(len(bins)-1):\n",
    "        total_count = sum((np.array(quality_scores) >= bins[i]) & (np.array(quality_scores) < bins[i+1]))\n",
    "        selected_count = sum((selected_scores >= bins[i]) & (selected_scores < bins[i+1]))\n",
    "        print(f\"  {bins[i]:3d}-{bins[i+1]:3d}점: 전체 {total_count:4,}개, 선택 {selected_count:4,}개\")\n",
    "\n",
    "    print(f\"\\n기준별 평균 점수 (선택된 쌍):\")\n",
    "    criteria_names = {\n",
    "        'zero': '0 비율',\n",
    "        'cv': '변동계수',\n",
    "        'spike': '스파이크',\n",
    "        'stability': '안정성',\n",
    "        'trend': '트렌드 일치',\n",
    "        'direction': '방향 일치도',\n",
    "        'significance': '통계적 유의성',\n",
    "        'autocorr': '자기상관',\n",
    "        'outlier': '이상치 비율',\n",
    "        'normality': '정규성'\n",
    "    }\n",
    "\n",
    "    selected_indices = pairs_with_score[pairs_with_score['quality_score'] >= threshold].index\n",
    "    selected_detailed = [detailed_scores[i] for i in selected_indices if i < len(detailed_scores)]\n",
    "\n",
    "    for key, name in criteria_names.items():\n",
    "        scores = [s.get(key, 0) for s in selected_detailed if s]\n",
    "        if scores:\n",
    "            if key == 'stability':\n",
    "                max_score = 15\n",
    "            elif key == 'normality':\n",
    "                max_score = 5\n",
    "            else:\n",
    "                max_score = 10\n",
    "\n",
    "            print(f\"  {name:15s}: 평균 {np.mean(scores):5.1f}점 (최대 {max_score}점)\")\n",
    "\n",
    "    # 품질 점수 컬럼 제거해서 반환\n",
    "    pairs_for_model = pairs_clean.drop(columns=['quality_score'], errors='ignore')\n",
    "    return pairs_for_model\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. 조건부 Weight 피처\n",
    "# =============================================================================\n",
    "\n",
    "def create_conditional_weight_features(value, weight, value_series):\n",
    "    \"\"\"조건부 Weight 피처 4개\"\"\"\n",
    "    value_positive = value_series[value_series > 0]\n",
    "    if len(value_positive) > 0:\n",
    "        value_median = np.median(value_positive)\n",
    "        value_q75 = np.percentile(value_positive, 75)\n",
    "    else:\n",
    "        value_median = 0.0\n",
    "        value_q75 = 0.0\n",
    "\n",
    "    if value > value_median:\n",
    "        weight_for_high_value = weight * 1.5\n",
    "    else:\n",
    "        weight_for_high_value = 0.0\n",
    "\n",
    "    weight_value_ratio = weight * (value / (value_median + 1))\n",
    "    exp_weighted = weight * (1 - np.exp(-value / (value_median + 1)))\n",
    "\n",
    "    if value > value_q75:\n",
    "        weight_importance = weight * 1.5\n",
    "    elif value > value_median:\n",
    "        weight_importance = weight * 1.0\n",
    "    else:\n",
    "        weight_importance = weight * 0.1\n",
    "\n",
    "    return {\n",
    "        'weight_for_high_value': weight_for_high_value,\n",
    "        'weight_value_ratio': weight_value_ratio,\n",
    "        'exp_weighted': exp_weighted,\n",
    "        'weight_importance': weight_importance\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. 학습 데이터 구축 (32개 피처, log1p target)\n",
    "# =============================================================================\n",
    "\n",
    "def build_training_data_optimized(pivot, pivot_weight, pairs, months_dt):\n",
    "    \"\"\"32개 피처 학습 데이터 생성 (log1p(target))\"\"\"\n",
    "    months = months_dt\n",
    "    n_months = len(months)\n",
    "    rows = []\n",
    "\n",
    "    for row in pairs.itertuples(index=False):\n",
    "        leader = row.leading_item_id\n",
    "        follower = row.following_item_id\n",
    "        lag = int(row.best_lag)\n",
    "        corr = float(row.max_corr)\n",
    "\n",
    "        if leader not in pivot.index or follower not in pivot.index:\n",
    "            continue\n",
    "        if leader not in pivot_weight.index or follower not in pivot_weight.index:\n",
    "            continue\n",
    "\n",
    "        a_series = pivot.loc[leader].values.astype(float)\n",
    "        b_series = pivot.loc[follower].values.astype(float)\n",
    "        b_weight_series = pivot_weight.loc[follower].values.astype(float)\n",
    "\n",
    "        # 최소 12개월 + lag 만큼 확보\n",
    "        for t in range(max(lag, 12), n_months - 1):\n",
    "            b_t = b_series[t]\n",
    "            b_t_1 = b_series[t - 1]\n",
    "            b_t_2 = b_series[t - 2]\n",
    "            b_t_12 = b_series[t - 12]\n",
    "            a_t_lag = a_series[t - lag]\n",
    "            b_t_plus_1 = b_series[t + 1]\n",
    "            b_weight_t = b_weight_series[t]\n",
    "            target_month = months[t + 1].month\n",
    "\n",
    "            # A(leader) 관련\n",
    "            a_t_lag_diff = a_series[t - lag] - a_series[t - lag - 1]\n",
    "            a_momentum = a_series[t - lag] / (a_series[t - lag - 3] + 1)\n",
    "            a_roll_mean_3 = (a_series[t-lag] + a_series[t-lag-1] + a_series[t-lag-2]) / 3\n",
    "\n",
    "            # B(follower) 자기회귀/변동\n",
    "            b_diff = b_t - b_t_1\n",
    "            b_pct_change = (b_t - b_t_1) / (b_t_1 + 1)\n",
    "            b_yoy_growth = (b_t - b_t_12) / (b_t_12 + 1)\n",
    "            b_roll_mean_3 = (b_series[t] + b_series[t-1] + b_series[t-2]) / 3\n",
    "            b_roll_mean_6 = b_series[t-5:t+1].mean()\n",
    "            b_roll_std_3 = np.std([b_series[t], b_series[t-1], b_series[t-2]])\n",
    "\n",
    "            # A-B 관계\n",
    "            ab_ratio = a_t_lag / (b_t + 1)\n",
    "            ab_diff = a_t_lag - b_t\n",
    "            corr_weighted_a = a_t_lag * corr\n",
    "\n",
    "            # 추가 시계열 특징\n",
    "            b_cv = b_roll_std_3 / (b_roll_mean_3 + 1)\n",
    "            b_acceleration = (b_t - b_t_1) - (b_t_1 - b_t_2)\n",
    "            b_yoy_ratio = b_t / (b_t_12 + 1)\n",
    "            b_ma_ratio = b_roll_mean_3 / (b_roll_mean_6 + 1)\n",
    "            recent_12 = b_series[max(0, t-11):t+1]\n",
    "            b_percentile_rank = (b_t >= recent_12).sum() / len(recent_12)\n",
    "\n",
    "            # 캘린더\n",
    "            quarter = (target_month - 1) // 3 + 1\n",
    "            is_year_end = 1 if target_month in [11, 12, 1] else 0\n",
    "            lag_weight = 1 / (1 + lag)\n",
    "\n",
    "            # weight 기반 피처\n",
    "            b_weight_features = create_conditional_weight_features(b_t, b_weight_t, b_series)\n",
    "\n",
    "            rows.append({\n",
    "                \"b_t\": b_t,\n",
    "                \"b_t_1\": b_t_1,\n",
    "                \"b_t_12\": b_t_12,\n",
    "                \"a_t_lag\": a_t_lag,\n",
    "                \"max_corr\": corr,\n",
    "                \"best_lag\": float(lag),\n",
    "                \"month\": float(target_month),\n",
    "\n",
    "                \"a_t_lag_diff\": a_t_lag_diff,\n",
    "                \"a_momentum\": a_momentum,\n",
    "                \"a_roll_mean_3\": a_roll_mean_3,\n",
    "\n",
    "                \"b_diff\": b_diff,\n",
    "                \"b_pct_change\": b_pct_change,\n",
    "                \"b_yoy_growth\": b_yoy_growth,\n",
    "                \"b_roll_mean_3\": b_roll_mean_3,\n",
    "                \"b_roll_mean_6\": b_roll_mean_6,\n",
    "                \"b_roll_std_3\": b_roll_std_3,\n",
    "\n",
    "                \"ab_ratio\": ab_ratio,\n",
    "                \"ab_diff\": ab_diff,\n",
    "                \"corr_weighted_a\": corr_weighted_a,\n",
    "\n",
    "                \"b_cv\": b_cv,\n",
    "                \"b_acceleration\": b_acceleration,\n",
    "                \"b_yoy_ratio\": b_yoy_ratio,\n",
    "                \"b_ma_ratio\": b_ma_ratio,\n",
    "                \"b_percentile_rank\": b_percentile_rank,\n",
    "\n",
    "                \"quarter\": float(quarter),\n",
    "                \"is_year_end\": is_year_end,\n",
    "                \"lag_weight\": lag_weight,\n",
    "\n",
    "                \"b_weight_for_high_value\": b_weight_features['weight_for_high_value'],\n",
    "                \"b_weight_value_ratio\": b_weight_features['weight_value_ratio'],\n",
    "                \"b_exp_weighted\": b_weight_features['exp_weighted'],\n",
    "                \"b_weight_importance\": b_weight_features['weight_importance'],\n",
    "\n",
    "                # 타깃: log1p\n",
    "                \"target\": np.log1p(b_t_plus_1),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 5. 예측 함수 (LGBM + XGB 앙상블, expm1 역변환)\n",
    "# =============================================================================\n",
    "\n",
    "def predict_optimized(pivot, pivot_weight, pairs, model_lgb, model_xgb, months_dt):\n",
    "    \"\"\"32개 피처 예측 + 앙상블\"\"\"\n",
    "    months = months_dt\n",
    "    n_months = len(months)\n",
    "    t_last = n_months - 1\n",
    "    t_prev = n_months - 2\n",
    "    preds = []\n",
    "    target_month = months[-1].month + 1 if months[-1].month < 12 else 1\n",
    "\n",
    "    for row in tqdm(pairs.itertuples(index=False), desc=\"예측\"):\n",
    "        leader = row.leading_item_id\n",
    "        follower = row.following_item_id\n",
    "        lag = int(row.best_lag)\n",
    "        corr = float(row.max_corr)\n",
    "\n",
    "        if leader not in pivot.index or follower not in pivot.index:\n",
    "            continue\n",
    "        if leader not in pivot_weight.index or follower not in pivot_weight.index:\n",
    "            continue\n",
    "\n",
    "        a_series = pivot.loc[leader].values.astype(float)\n",
    "        b_series = pivot.loc[follower].values.astype(float)\n",
    "        b_weight_series = pivot_weight.loc[follower].values.astype(float)\n",
    "\n",
    "        if t_last - lag < 0 or t_last < 12:\n",
    "            continue\n",
    "\n",
    "        b_t = b_series[t_last]\n",
    "        b_t_1 = b_series[t_prev]\n",
    "        b_t_2 = b_series[t_prev - 1]\n",
    "        b_t_12 = b_series[t_last - 12]\n",
    "        a_t_lag = a_series[t_last - lag]\n",
    "        b_weight_t = b_weight_series[t_last]\n",
    "\n",
    "        a_t_lag_diff = a_series[t_last - lag] - a_series[t_last - lag - 1]\n",
    "        a_momentum = a_series[t_last - lag] / (a_series[t_last - lag - 3] + 1)\n",
    "        a_roll_mean_3 = (a_series[t_last-lag] + a_series[t_last-lag-1] + a_series[t_last-lag-2]) / 3\n",
    "\n",
    "        b_diff = b_t - b_t_1\n",
    "        b_pct_change = (b_t - b_t_1) / (b_t_1 + 1)\n",
    "        b_yoy_growth = (b_t - b_t_12) / (b_t_12 + 1)\n",
    "        b_roll_mean_3 = (b_series[t_last] + b_series[t_last-1] + b_series[t_last-2]) / 3\n",
    "        b_roll_mean_6 = b_series[t_last-5:t_last+1].mean()\n",
    "        b_roll_std_3 = np.std([b_series[t_last], b_series[t_last-1], b_series[t_last-2]])\n",
    "\n",
    "        ab_ratio = a_t_lag / (b_t + 1)\n",
    "        ab_diff = a_t_lag - b_t\n",
    "        corr_weighted_a = a_t_lag * corr\n",
    "\n",
    "        b_cv = b_roll_std_3 / (b_roll_mean_3 + 1)\n",
    "        b_acceleration = (b_t - b_t_1) - (b_t_1 - b_t_2)\n",
    "        b_yoy_ratio = b_t / (b_t_12 + 1)\n",
    "        b_ma_ratio = b_roll_mean_3 / (b_roll_mean_6 + 1)\n",
    "        recent_12 = b_series[max(0, t_last-11):t_last+1]\n",
    "        b_percentile_rank = (b_t >= recent_12).sum() / len(recent_12)\n",
    "\n",
    "        quarter = (target_month - 1) // 3 + 1\n",
    "        is_year_end = 1 if target_month in [11, 12, 1] else 0\n",
    "        lag_weight = 1 / (1 + lag)\n",
    "\n",
    "        b_weight_features = create_conditional_weight_features(b_t, b_weight_t, b_series)\n",
    "\n",
    "        X_test = np.array([[\n",
    "\n",
    "            b_t, b_t_1, b_t_12, a_t_lag, corr, float(lag), float(target_month),\n",
    "\n",
    "            a_t_lag_diff, a_momentum, a_roll_mean_3,\n",
    "\n",
    "            b_diff, b_pct_change, b_yoy_growth,\n",
    "            b_roll_mean_3, b_roll_mean_6, b_roll_std_3,\n",
    "\n",
    "            ab_ratio, ab_diff, corr_weighted_a,\n",
    "\n",
    "            b_cv, b_acceleration, b_yoy_ratio, b_ma_ratio, b_percentile_rank,\n",
    "\n",
    "            float(quarter), is_year_end, lag_weight,\n",
    "\n",
    "            b_weight_features['weight_for_high_value'],\n",
    "            b_weight_features['weight_value_ratio'],\n",
    "            b_weight_features['exp_weighted'],\n",
    "            b_weight_features['weight_importance']\n",
    "        ]])\n",
    "\n",
    "        pred_lgb = model_lgb.predict(X_test)[0]\n",
    "        pred_xgb = model_xgb.predict(X_test)[0]\n",
    "        y_pred_log = 0.7 * pred_lgb + 0.3 * pred_xgb\n",
    "        y_pred = np.expm1(y_pred_log)\n",
    "        y_pred = max(0.0, float(y_pred))\n",
    "        y_pred = int(round(y_pred))\n",
    "\n",
    "        preds.append({\n",
    "            \"leading_item_id\": leader,\n",
    "            \"following_item_id\": follower,\n",
    "            \"value\": y_pred,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(preds)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 6. 메인 실행\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" DTW 기반 pairs → 2,700개 품질 필터 → 32피처 LGBM+XGB 앙상블\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # 1) Corr + DTW 기반 pair 추출\n",
    "    pairs_with_dtw = find_comovement_pairs_with_dtw(\n",
    "        pivot=pivot,\n",
    "        max_lag=9,\n",
    "        min_nonzero=12,\n",
    "        corr_threshold=0.33,\n",
    "        score_threshold=40,\n",
    "        use_dtw=True,\n",
    "        dtw_threshold=1.0\n",
    "    )\n",
    "\n",
    "    # 2) 2,700개 목표 노이즈 제거\n",
    "    pairs_for_model = remove_noise_pairs_for_2700(\n",
    "        pairs=pairs_with_dtw,\n",
    "        pivot=pivot,\n",
    "        target_count=2700\n",
    "    )\n",
    "\n",
    "    print(f\"\\n최종 사용 쌍: {len(pairs_for_model)}개\")\n",
    "\n",
    "    # 3) 학습 데이터 생성\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"학습 데이터 생성\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    df_train_model = build_training_data_optimized(pivot, pivot_weight, pairs_for_model, months_dt)\n",
    "    print(f\"✓ 학습 데이터: {df_train_model.shape}\")\n",
    "\n",
    "    feature_cols = [\n",
    "        'b_t', 'b_t_1', 'b_t_12', 'a_t_lag', 'max_corr', 'best_lag', 'month',\n",
    "        'a_t_lag_diff', 'a_momentum', 'a_roll_mean_3',\n",
    "        'b_diff', 'b_pct_change', 'b_yoy_growth', 'b_roll_mean_3', 'b_roll_mean_6', 'b_roll_std_3',\n",
    "        'ab_ratio', 'ab_diff', 'corr_weighted_a',\n",
    "        'b_cv', 'b_acceleration', 'b_yoy_ratio', 'b_ma_ratio', 'b_percentile_rank',\n",
    "        'quarter', 'is_year_end', 'lag_weight',\n",
    "        'b_weight_for_high_value', 'b_weight_value_ratio', 'b_exp_weighted', 'b_weight_importance'\n",
    "    ]\n",
    "\n",
    "    if not df_train_model.empty:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"모델 학습\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        train_X = df_train_model[feature_cols].values\n",
    "        train_y = df_train_model[\"target\"].values\n",
    "\n",
    "        print(f\"학습 샘플: {len(train_X):,}, 피처: {len(feature_cols)}\")\n",
    "\n",
    "        # LGBM\n",
    "        model_lgb = LGBMRegressor(\n",
    "            random_state=42,\n",
    "            n_estimators=700,\n",
    "            learning_rate=0.01,\n",
    "            max_depth=8,\n",
    "            num_leaves=50,\n",
    "            min_child_samples=20,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=0.1,\n",
    "            min_split_gain=0.01\n",
    "        )\n",
    "\n",
    "        print(\"\\nLGBM 학습 중...\")\n",
    "        model_lgb.fit(train_X, train_y)\n",
    "        print(\"✓ LGBM 완료\")\n",
    "\n",
    "        # XGBoost\n",
    "        model_xgb = XGBRegressor(\n",
    "            random_state=42,\n",
    "            n_estimators=900,\n",
    "            learning_rate=0.02,\n",
    "            max_depth=8,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=0.1,\n",
    "            tree_method='hist'\n",
    "        )\n",
    "\n",
    "        print(\"XGBoost 학습 중...\")\n",
    "        model_xgb.fit(train_X, train_y)\n",
    "        print(\"✓ XGBoost 완료\")\n",
    "\n",
    "        # 4) 예측\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"예측\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        submission = predict_optimized(\n",
    "            pivot, pivot_weight,\n",
    "            pairs_for_model,\n",
    "            model_lgb, model_xgb,\n",
    "            months_dt\n",
    "        )\n",
    "        print(f\"\\n✓ 예측 완료: {len(submission)}개\")\n",
    "\n",
    "        os.makedirs(\"_result\", exist_ok=True)\n",
    "        date_str = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        out_path = f\"_result/jh_2700dtw_{date_str}.csv\"\n",
    "        submission.to_csv(out_path, index=False)\n",
    "\n",
    "        print(\"\\n 최종 완료!\")\n",
    "        print(f\"  - 저장 위치: {out_path}\")\n",
    "        print(\"  - Stage1: Corr+DTW pairs → 2,700개 품질 필터\")\n",
    "        print(\"  - Stage2: 32개 피처 + log1p target\")\n",
    "        print(\"  - Stage3: LGBM + XGB 앙상블 (0.7 / 0.3)\")\n",
    "        print(\"=\"*80)\n",
    "    else:\n",
    "        print(\"학습 데이터가 비어 있습니다. 파라미터나 필터 조건을 확인하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcbde33-b88f-4f76-9b01-04b9bbe32ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_new_env)",
   "language": "python",
   "name": "my_new_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
